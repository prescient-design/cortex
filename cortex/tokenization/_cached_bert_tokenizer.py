import torch
from cachetools import LRUCache, cached
from torch import LongTensor, Tensor
from transformers import BertTokenizerFast


class CachedBertTokenizerFast(BertTokenizerFast):
    """
    This class is a wrapper around the BertTokenizerFast class from the transformers library.
    It adds an additional cached encoding method for faster runtimes on smaller datasets.
    It also provides attributes indicating which tokens can be corrupted and sampled by denoising models,
    and a convenience method for getting a mask of corruptible tokens from a sequence of token ids.
    """

    def __init__(
        self,
        vocab_file: str = None,
        tokenizer_file: str = None,
        do_lower_case: bool = False,
        unk_token: str = "[UNK]",
        sep_token: str = "[SEP]",
        pad_token: str = "[PAD]",
        cls_token: str = "[CLS]",
        mask_token: str = "[MASK]",
        raise_unk_exception: bool = False,
        add_cls_and_sep: bool = False,
        **kwargs,
    ):
        super().__init__(
            vocab_file=vocab_file,
            tokenizer_file=tokenizer_file,
            do_lower_case=do_lower_case,
            unk_token=unk_token,
            sep_token=sep_token,
            pad_token=pad_token,
            cls_token=cls_token,
            mask_token=mask_token,
            **kwargs,
        )
        self.padding_idx = self.vocab[pad_token]
        self.masking_idx = self.vocab[mask_token]

        # prevent utility token input corruption
        utility_tokens = [
            unk_token,
            sep_token,
            pad_token,
            cls_token,
            mask_token,
        ]
        self.corruption_vocab_excluded = set(utility_tokens)
        self.sampling_vocab_excluded = set(utility_tokens)
        self._raise_unk_exception = raise_unk_exception
        self._add_cls_and_sep = add_cls_and_sep

    @property
    def corruption_vocab_included(self):
        """
        Tokens that can be corrupted by denoising models.
        """
        return set(self.vocab.keys()) - self.corruption_vocab_excluded

    @property
    def sampling_vocab_included(self):
        """
        Tokens that can be sampled by denoising models.
        """
        return set(self.vocab.keys()) - self.sampling_vocab_excluded

    def decode(self, token_ids: LongTensor, skip_special_tokens: bool = True):
        """
        Decode a sequence of token ids into a string.
        """
        token_list = self.convert_ids_to_tokens(token_ids)
        if skip_special_tokens:
            token_list = [t for t in token_list if t not in self.all_special_tokens]
        return " ".join(token_list)

    @cached(cache=LRUCache(maxsize=int(1e6)))
    def cached_encode(self, text: str):
        """
        Cached tokenizer call, for faster runtimes on smaller datasets.
        """
        tokens = text.split(" ")
        if self._add_cls_and_sep:
            tokens = [self.cls_token] + tokens + [self.sep_token]
        res = self.convert_tokens_to_ids(tokens)
        unknown_tokens = [t for t, idx in zip(tokens, res) if idx == self.unk_token_id]
        if len(unknown_tokens) > 0 and self._raise_unk_exception:
            raise RuntimeError(f"Input: {text} contains unknown tokens: {unknown_tokens}")
        return res

    def get_corruptible_mask(self, token_batch: LongTensor) -> Tensor:
        """
        Args:
            token_batch: a batch of token ids (LongTensor).
        Returns:
            a boolean mask tensor of corruptible tokens (corrupt if True).
        """
        excluded_idxs = (
            torch.tensor([self.vocab[tok] for tok in self.corruption_vocab_excluded]).view(-1, 1, 1).to(token_batch)
        )
        is_corruptible = token_batch.ne(excluded_idxs).prod(dim=0).bool()
        return is_corruptible
