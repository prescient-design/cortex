defaults:
  - _self_
  - logging: default
  - tasks:
    - protein_property/log_fluorescence_hf

# HuggingFace protein model training configuration
job_name: hf_protein_model

# Tree configuration
tree:
  _target_: cortex.model.tree.NeuralTreeLightningV2
  root_nodes:
    _target_: torch.nn.ModuleDict
  trunk_node: null
  branch_nodes:
    _target_: torch.nn.ModuleDict
  leaf_nodes:
    _target_: torch.nn.ModuleDict
seed: 42
num_workers: 4  # Reduced for Mac
download_datasets: true
dataset_root_dir: ${hydra:runtime.cwd}/data
data_dir: ${hydra:runtime.cwd}/data
save_ckpt: true
ckpt_file: model.ckpt
ckpt_cfg: model.yaml
warnings_filter: default

# Wandb config
wandb_mode: offline

# Model configuration - using tiny BERT for Mac
roots:
  protein_seq:
    _target_: cortex.model.root.HuggingFaceRoot
    model_name_or_path: prajjwal1/bert-tiny  # 4.4M params instead of 420M
    # model_name_or_path: facebook/esm2_t30_150M_UR50D
    pooling_strategy: none  # Keep sequence dimension for Conv1dBranch
    freeze_pretrained: false
    # cropped_max_len: 256

trunk:
  _target_: cortex.model.trunk.SumTrunk
  out_dim: 128  # Smaller for tiny model

branches:
  protein_property:
    _target_: cortex.model.branch.Conv1dBranch
    out_dim: 64  # Smaller
    num_blocks: 1  # Fewer blocks
    kernel_size: 5
    dilation_base: 1
    channel_dim: 128  # Smaller
    channel_dim_mult: 1
    dropout_p: 0.1

# Task configuration using HuggingFace datasets
tasks:
  protein_property:
    log_fluorescence_hf:
      ensemble_size: 1

# Training configuration
fit:
  batch_size: 128  # Smaller batch size for Mac
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4  # Higher LR for faster convergence in demo
    weight_decay: 0.0
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 10  # Shorter schedule
    eta_min: 1e-5

trainer:
  _target_: lightning.pytorch.Trainer
  max_epochs: 64  # Just 2 epochs for demo
  accelerator: gpu
  devices: 1
  precision: 16-mixed  # Full precision on Mac
  # gradient_clip_val: 1.0  # Not supported with manual optimization
  accumulate_grad_batches: 1
  log_every_n_steps: 10
  val_check_interval: 1.0
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  num_sanity_val_steps: 0
