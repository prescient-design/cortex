cortex.model.tree._seq_model_tree
=================================

.. py:module:: cortex.model.tree._seq_model_tree


Classes
-------

.. autoapisummary::

   cortex.model.tree._seq_model_tree.SequenceModelTree


Functions
---------

.. autoapisummary::

   cortex.model.tree._seq_model_tree._infer_root_key
   cortex.model.tree._seq_model_tree.get_param_prefixes
   cortex.model.tree._seq_model_tree.split_data
   cortex.model.tree._seq_model_tree.split_list


Module Contents
---------------

.. py:class:: SequenceModelTree(root_nodes: Optional[torch.nn.ModuleDict] = None, trunk_node: Optional[torch.nn.Module] = None, branch_nodes: Optional[torch.nn.ModuleDict] = None, leaf_nodes: Optional[torch.nn.ModuleDict] = None, fit_cfg: Optional[omegaconf.DictConfig] = None)

   Bases: :py:obj:`cortex.model.tree.NeuralTree`, :py:obj:`lightning.LightningModule`


   Compute tree graph composed of root, trunk, branch, and leaf neural network nodes


   .. py:attribute:: _train_state_dict
      :value: None



   .. py:attribute:: _eval_state_dict
      :value: None



   .. py:attribute:: _w_avg_step_count
      :value: 1



   .. py:attribute:: training_step_outputs
      :value: []



   .. py:attribute:: validation_step_outputs
      :value: []



   .. py:attribute:: automatic_optimization
      :value: False



   .. py:method:: train(*args, **kwargs)


   .. py:method:: eval(*args, **kwargs)


   .. py:method:: get_dataloader(split='train')


   .. py:method:: training_step(batch: dict, batch_idx: int, dataloader_idx: Optional[int] = None)


   .. py:method:: training_step_end(step_metrics)


   .. py:method:: on_train_epoch_end()


   .. py:method:: _weight_average_update(w_avg_step_count: int) -> int


   .. py:method:: on_train_epoch_start()


   .. py:method:: on_fit_start() -> None


   .. py:method:: configure_optimizers()


   .. py:method:: evaluate(*args, **kwargs)


   .. py:method:: validation_step(batch: dict, batch_idx: int, dataloader_idx: Optional[int] = None)


   .. py:method:: on_validation_epoch_start()


   .. py:method:: on_validation_epoch_end()


   .. py:method:: finetune(cfg: omegaconf.DictConfig, task_dict: dict, log_prefix: Optional[str] = '', ckpt_file: Optional[str] = None, ckpt_cfg: Optional[str] = None, tune_branches: bool = True, reinitialize_leaves: bool = False, linear_probing: bool = False, **kwargs) -> list[Dict]


   .. py:method:: add_task_nodes(branch_key: str, task_key: str, task_cfg: omegaconf.DictConfig, branch_cfg: Optional[omegaconf.DictConfig] = None, data_dir: str = './data') -> None


   .. py:method:: initialize_leaves(task_dict) -> None


   .. py:method:: build_tree(cfg: omegaconf.DictConfig, skip_task_setup: bool = False) -> dict


   .. py:method:: predict(data: collections.OrderedDict, batch_limit: Optional[int] = None, predict_tasks: Optional[list[str]] = None, format_outputs: bool = True, cpu_offload: bool = True) -> dict[str, torch.Tensor]

      :param predict_inputs: array of inputs (use `AbModel.df_to_pred_inputs` to convert a dataframe)

      :returns: dict[str, torch.Tensor] of task prediction outputs for `predict_inputs`.
      :rtype: predict_out



   .. py:method:: prediction_metrics(task_dict, predict_out, predict_targets, log_prefix='')


   .. py:method:: _predict_batch(data: pandas.DataFrame, format_outputs: bool = True, task_keys: Optional[list[str]] = None) -> dict

      :param aa_seqs: [(AbHC, AbLC, Ag), ...]
      :param predict_tasks: list of task keys indicating which task predictions are required.
                            Defaults to `None`, which indicates all tasks are required without enumeration.



   .. py:method:: format_task_outputs(task_out, task_keys, task_leaves)


   .. py:method:: call_from_str_array(str_array, root_key: Optional[str] = None, leaf_keys: Optional[list[str]] = None, **kwargs)


   .. py:method:: call_from_tok_idxs(tok_idxs: torch.LongTensor, root_key: Optional[str] = None, leaf_keys: Optional[list[str]] = None, **kwargs)


   .. py:method:: call_from_tok_embs(tok_embs: torch.FloatTensor, root_key: Optional[str] = None, leaf_keys: Optional[list[str]] = None, **kwargs)


   .. py:method:: get_tokenizer(root_key: Optional[str] = None)


.. py:function:: _infer_root_key(root_nodes)

.. py:function:: get_param_prefixes(tree_outputs)

.. py:function:: split_data(data: collections.OrderedDict[str, int | list[Any]], batch_size: int)

   Split a dictionary into n chunks with size at most batch_size.
   If batch_size is not a divisor of the dictionary element length, the last chunk will be smaller.
   :param data: dict to split
   :param batch_size: max size of each chunk

   :returns: list of dict chunks


.. py:function:: split_list(lst: list[Any], batch_size: int) -> list[list[Any]]

   Split a list into chunks with size at most batch_size.
   If batch_size is not a divisor of the list length, the last chunk will be smaller.
   :param lst: list to split
   :param batch_size: max size of each chunk

   :returns: list of list chunks


