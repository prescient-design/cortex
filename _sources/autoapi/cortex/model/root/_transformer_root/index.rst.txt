cortex.model.root._transformer_root
===================================

.. py:module:: cortex.model.root._transformer_root


Classes
-------

.. autoapisummary::

   cortex.model.root._transformer_root.TransformerRootOutput
   cortex.model.root._transformer_root.TransformerRoot


Module Contents
---------------

.. py:class:: TransformerRootOutput

   Output of TransforerEncoderRoot.


   .. py:attribute:: root_features
      :type:  torch.Tensor


   .. py:attribute:: padding_mask
      :type:  torch.Tensor


   .. py:attribute:: corrupt_frac
      :type:  Optional[torch.Tensor]
      :value: None



   .. py:attribute:: src_tok_idxs
      :type:  Optional[torch.LongTensor]
      :value: None



   .. py:attribute:: tgt_tok_idxs
      :type:  Optional[torch.LongTensor]
      :value: None



   .. py:attribute:: src_tok_embs
      :type:  Optional[torch.Tensor]
      :value: None



   .. py:attribute:: is_corrupted
      :type:  Optional[torch.Tensor]
      :value: None



.. py:class:: TransformerRoot(tokenizer_transform: cortex.transforms.HuggingFaceTokenizerTransform, max_len: int, out_dim: int = 64, embed_dim: int = 64, channel_dim: int = 256, num_blocks: int = 2, num_heads: int = 4, is_causal: bool = False, dropout_prob: float = 0.0, pos_encoding: bool = True, train_transforms=None, eval_transforms=None, corruption_process: Optional[cortex.corruption.CorruptionProcess] = None, **kwargs)

   Bases: :py:obj:`cortex.model.root.RootNode`


   A root node transforming an array of discrete sequences to an array of continuous sequence embeddings


   .. py:attribute:: tokenizer


   .. py:attribute:: vocab_size


   .. py:attribute:: max_len


   .. py:attribute:: pad_tok_idx


   .. py:attribute:: embed_dim
      :value: 64



   .. py:attribute:: num_blocks
      :value: 2



   .. py:attribute:: train_transform


   .. py:attribute:: eval_transform


   .. py:attribute:: corruption_process
      :value: None



   .. py:method:: initialize_weights(**kwargs)


   .. py:method:: get_token_embedding(tok_idx: int)


   .. py:property:: device


   .. py:method:: init_seq(inputs: Optional[Union[numpy.ndarray, torch.Tensor]] = None, seq_array: Optional[numpy.ndarray] = None, tgt_tok_idxs: Optional[torch.LongTensor] = None, src_tok_embs: Optional[torch.Tensor] = None, corrupt_frac: float = 0.0, **kwargs)


   .. py:method:: tokenize_seq(seq_array: Optional[numpy.ndarray] = None, tgt_tok_idxs: Optional[torch.LongTensor] = None, src_tok_embs: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, corrupt_frac: Union[float, torch.Tensor] = 0.0, is_corrupted: Optional[torch.Tensor] = None, corruption_allowed: Optional[torch.Tensor] = None)


   .. py:method:: embed_seq(src_tok_idxs: Optional[torch.LongTensor] = None, src_tok_embs: Optional[torch.Tensor] = None, corrupt_frac: Union[float, torch.Tensor] = 0.0, is_corrupted: Optional[torch.Tensor] = None, corruption_allowed: Optional[torch.Tensor] = None, normalize_embeds: bool = True)


   .. py:method:: process_seq(src_tok_embs: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None)


   .. py:method:: forward(inputs: Optional[Union[numpy.ndarray, torch.Tensor]] = None, seq_array: Optional[numpy.ndarray] = None, tgt_tok_idxs: Optional[torch.LongTensor] = None, src_tok_embs: Optional[torch.Tensor] = None, padding_mask: Optional[torch.Tensor] = None, corrupt_frac: Union[float, torch.Tensor] = 0.0, is_corrupted: Optional[torch.Tensor] = None, corruption_allowed: Optional[torch.Tensor] = None, **kwargs) -> TransformerRootOutput

      :param seq_array: (batch_size,) array of discrete sequences (e.g. text strings)

      :returns: {'root_features': torch.Tensor, 'padding_mask': torch.Tensor}
      :rtype: outputs



