cortex.model._infer_with_model
==============================

.. py:module:: cortex.model._infer_with_model


Functions
---------

.. autoapisummary::

   cortex.model._infer_with_model.infer_with_model


Module Contents
---------------

.. py:function:: infer_with_model(data: pandas.DataFrame, model: Optional[torch.nn.Module] = None, cfg_fpath: Optional[str] = None, weight_fpath: Optional[str] = None, batch_limit: int = 32, cpu_offload: bool = True, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> dict[str, numpy.ndarray]

   A functional interface for inference with a cortex model.

   Usage:

   ```python title="Example of inference with a cortex model checkpoint."
   from cortex.model import infer_with_model

   ckpt_dir = <TODO>
   ckpt_name = <TODO>
   predictions = infer_with_model(
       data=df,
       cfg_fpath=f"{ckpt_dir}/{ckpt_name}.yaml",
       weight_fpath=f"{ckpt_dir}/{ckpt_name}.pt",
   )
   ```

   :param data: A dataframe containing the sequences to predict on.
   :type data: pd.DataFrame
   :param cfg_fpath: The path to the Hydra config file on S3.
   :type cfg_fpath: str
   :param weight_fpath: The path to the PyTorch model weights on S3.
   :type weight_fpath: str
   :param batch_limit: The maximum number of sequences to predict on at once. Defaults to 32.
   :type batch_limit: int, optional
   :param cpu_offload: Whether to use cpu offload.
                       If true, will run prediction with cpu offload. Defaults to True
   :type cpu_offload: bool, optional
   :param device: The device to run the model on. Defaults to None.
   :type device: torch.device, optional
   :param dtype: The dtype to run the model on. Defaults to None.
   :type dtype: torch.dtype, optional

   :returns: A dict of NumPy arrays of the predictions.
   :rtype: dict[str, np.ndarray]


